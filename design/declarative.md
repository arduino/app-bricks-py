# Declarative style design

## Requirements

1. Declarative programming style: the user declaratively defines the pipeline structure, e.g. via chaining or composition of the step classes.
2. Java Stream API-like: handles parallelism, backpressure, and synchronization automatically.
3. Invisible framework: the user code should not depend directly on the framework, if possible. The framework should wrap the logic and add asynchronicity.
4. Maximum freedom, simplicity, reusability: as a consequence, nodes should be easy to develop and usable outside the framework.
5. Encapsulation: the user defines "nodes of computation" that are plain Python classes unaware of the framework, of their asynchronous execution and of other nodes.
6. Single entry point: we want a single entry point, responsible for linking the nodes together, orchestration and handling parallelism, backpressure, and synchronization.
7. Simple API: the API should be easy to understand and implement.

## Additional requirements

1. Buffering <- covers the video streaming case, policy (block or block with spill-over after timeout (needed?) or spill-over when full)
2. Rate-limiting <- covers the case of the video camera rate (acquisition at a fixed rate) + API limits (block or spill-over when full)
3. Track the state of a node (processing, idle) <- covers the LLM "thinking" case
4. API ergonomics <- non-ergonomic source API? -> pipe.inject(...)?
5. Multiple outputs (LLM streaming) === for a message, potentially I could want to produce N messages -> return an array or an iterator?
6. Re-expose the basic board peripherals as default modules => Direct access to /dev? User and capabilities?
7. Don't undermine the imperative version
8. Branching / forking
9. Merging?
10. Execution statistics (latency, throughput, distributions etc.)

## Design

- Pipeline: the Pipeline class will take a sequence of source -> processor -> sink classes as input. The order of these classes will define the order in the processing pipeline.
- Source, Processor & Sink: since the framework shouldn't impose any inheritance or interface to implement, we need a convention for how the framework will interact with these classes. A simple approach is to expect each node to have a specific method that the framework can call to process an item. Let's call this method `produce` for sources, `process` for processors and `consume` for sinks. This method should take one input and return one output (or an exception).
- Data Flow: the framework will be responsible for passing data between the steps and should provide a way to capture the source data, process it in processors and consume the final output of the pipeline in the sink. This could involve using queues for internal communication and managing backpressure and asynchronicity. We also need to expose some sort of tuning of the backpressure strategy and adapters to manage the data model mismatch between these components.
- Boot sequence: the Pipeline class will need a method to start the processing. The source can be a class that naturally produces data when polled (by blocking) or an iterable of data. For establishing connections or sessions outside constructors, we'll need an optional `start` method on the step classes.
- Shutdown sequence: the Pipeline class will need a method to stop the processing. Since the steps might be stopped while processing or while blocked, we'll need an optional `stop` method on the step classes to handle resources cleanup and a way to handle exceptions that might be generated by the processing part.


## Example syntax
### Defaults
```python
pipe = Pipeline()
pipe.add_source(UserInputText())
pipe.add_processor(WeatherForecast())
pipe.add_processor(print)
pipe.add_sink(DBStore())
pipe.start()
```

### Adapters - explicit mapping
```python
def user_mapping_function(some_input: dict) -> dict:
    # User defined logic
    ...

pipe = Pipeline()
pipe.add_source(MQTTInput())
pipe.add_processor(JSONParser()) # To properly filter input - or
pipe.add_processor(user_mapping_function) # To let use inline a used defined function
pipe.add_processor(WeatherForecast())
pipe.add_processor(print)
pipe.add_sink(DBStore())
pipe.start()
```

### Adapters - inline mapping
```python
pipe = Pipeline()
pipe.add_source(UserInputText(), map = lambda x: x.strip())
pipe.add_processor(WeatherForecast(), map = lambda x: x.temperature_c, max_rate = "1/s")
pipe.add_processor(print)
pipe.add_sink(DBStore(), map = lambda x: { "temperature_c": x.temperature_c, "time": datetime.now() })
pipe.start()
```